{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains the first project for Information Retrieval 1 taught at the UvA. Code is made by Oscar Ligthart, Nicole Ferreira Silverio and Arend van Dormalen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER TO THEORETICAL QUESTION 1A\n",
    "\n",
    "The chance of a type 1 error ($\\alpha$) increases each time an experiment is repeated, if it's not corrected. The new $\\alpha$ for _m_ experiments is $1 − (1 − \\alpha)^m ≈ m\\alpha$. This is called the Family-wise error rate.\n",
    "\n",
    "ANSWER TO THEORETICAL QUESTION 1B,\n",
    "\n",
    "The chance of a type 1 error needs to be corrected. This can be done by the Bonferroni correction, which rejects the null hypothesis for any $p_i$ when it it lower than or equal to $\\frac{\\alpha}{m}$.\n",
    "      \n",
    "\n",
    "ANSWER TO THEORETICAL QUESTION 2\n",
    "\n",
    "Assume two ranked lists created by two different rankers. List $l1$ contains documents $d1$, $d2$ and $d3$ in that order. List $l2$ contains documents $d2$, $d3$ and $d4$ in that order. Now assume that the only relevant document is $d3$, which will therefore be clicked on most often. From our judgment, it is obvious that $l2$ is the most relevant list as it has placed $d3$ on a higher position. However, in Team Draft Interleaving, these algorithms will be evaluated as having equal performance.\n",
    "\n",
    "In this situation, $d3$ will always be the third item on the interleaved list. After the first coin flip, $d2$ will be removed from $l1$ as this document has already been supplied by $l2$.  At the second coin flip, $d3$ will be the next document for both lists. This causes the relevance for both lists to be the same, as they now both have the same chance of supplying the only relevant document to the interleaved list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "\n",
    "The following cell represents the creation of all possible relevance pairs E and P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import scipy.stats\n",
    "\n",
    "# first get the sequence options\n",
    "relevance = ['N', 'R', 'HR']\n",
    "options = list(itertools.product(relevance, repeat = 5))\n",
    "\n",
    "# create all possible pairs in sequence options\n",
    "pair_index = list(itertools.permutations(range(len(options)), 2))\n",
    "\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for temp_pair in pair_index:\n",
    "    pairs.append([options[temp_pair[0]], options[temp_pair[1]]])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline measures\n",
    "\n",
    "Each cell represents one of the following three offline measures: Average precision, nDCG, ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_precision(ranking):\n",
    "    rel = 0\n",
    "    AP_numerator = 0\n",
    "    # get amount of relevant documents\n",
    "    for i, doc in enumerate(ranking):\n",
    "        if doc == 'R' or doc == 'HR':\n",
    "            rel += 1\n",
    "            AP_numerator += rel/(i+1)\n",
    "            \n",
    "    return rel, AP_numerator\n",
    "\n",
    "# new dict for average precision for both P and E (key is pair, value is average precisions))\n",
    "AP_delta = {}\n",
    "\n",
    "# new dict for delta measures (AP, nDCG and ERR will be stored here per pair)\n",
    "delta_values = {}\n",
    "\n",
    "# get precision for all pairs\n",
    "for pair in pairs:\n",
    "    # first calculate numerator for average precision for P\n",
    "    P = pair[0]    \n",
    "    P_rel, P_AP_numerator = get_average_precision(P)\n",
    " \n",
    "    # now calculate numerator for average precision for E\n",
    "    E = pair[1]\n",
    "    E_rel, E_AP_numerator = get_average_precision(E)\n",
    "\n",
    "    # get total number of relevant documents returned from query\n",
    "    total_rel = P_rel + E_rel\n",
    "    \n",
    "    # calculate average precision for both P and E\n",
    "    P_AP = P_AP_numerator/total_rel\n",
    "    E_AP = E_AP_numerator/total_rel\n",
    "    \n",
    "    # store results in a dict\n",
    "    AP_delta[(P,E)] = E_AP - P_AP\n",
    "    \n",
    "    # store AP delta measures in list format in dict\n",
    "    delta_values[(P,E)] = [E_AP - P_AP]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### nDCG cell #####\n",
    "def get_nDCG(ranking):\n",
    "    DCG = 0\n",
    "    \n",
    "    # loop through ranking\n",
    "    for i, rank in enumerate(ranking):\n",
    "        # decide what the relative rank is\n",
    "        if rank == 'HR':\n",
    "            rel_r = 2\n",
    "        elif rank == 'R':\n",
    "            rel_r = 1\n",
    "        elif rank == 'N':\n",
    "            rel_r = 0\n",
    "        \n",
    "        DCG += (2**rel_r - 1)/(np.log2(1+(i+1)))\n",
    "    \n",
    "    return DCG\n",
    "\n",
    "# new dict for average precision for both P and E (key is pair, value is average precisions))\n",
    "nDCG_delta = {}\n",
    "\n",
    "counter = 0\n",
    "same_counter = 0\n",
    "lower_counter = 0\n",
    "\n",
    "# get nDCG for all pairs\n",
    "for pair in pairs:    \n",
    "    # first for P\n",
    "    P = pair[0]\n",
    "    P_DCG = get_nDCG(P)\n",
    "    \n",
    "    # then for E\n",
    "    E = pair[1]\n",
    "    E_DCG = get_nDCG(E)\n",
    "    \n",
    "    nDCG_delta[(P,E)] = E_DCG - P_DCG\n",
    "    \n",
    "    # add nDCG delta measure to dict\n",
    "    delta_values[(P,E)].append(E_DCG - P_DCG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### ERR cell #####\n",
    "\n",
    "def get_ERR(ranking):\n",
    "    \n",
    "    ERR = 0\n",
    "    p = 1\n",
    "    max_rel = 2\n",
    "    \n",
    "    # loop through ranking\n",
    "    for i, rank in enumerate(ranking):        \n",
    "        # start at second rank\n",
    "        if i != 0:            \n",
    "            # decide what the relative rank is\n",
    "            if rank == 'HR':\n",
    "                rel_r = 2\n",
    "            elif rank == 'R':\n",
    "                rel_r = 1\n",
    "            elif rank == 'N':\n",
    "                rel_r = 0\n",
    "\n",
    "            # Calculate R with the mapping function\n",
    "            R = (2**rel_r - 1)/(2**max_rel)\n",
    "\n",
    "            # Modify ERR value\n",
    "            ERR += p * (R/i)\n",
    "\n",
    "            # Modify p\n",
    "            p = p*(1-R)\n",
    "    \n",
    "    return ERR\n",
    "\n",
    "# new dict for ERR values for both P and E (key is pair, value is ERR value))\n",
    "ERR_delta = {}\n",
    "\n",
    "# get ERR for all pairs\n",
    "for pair in pairs:\n",
    "    \n",
    "    # first for P\n",
    "    P = pair[0]\n",
    "    P_ERR = get_ERR(P)\n",
    "    \n",
    "    # then for E\n",
    "    E = pair[1]\n",
    "    E_ERR = get_ERR(E)\n",
    "    \n",
    "    ERR_delta[(P,E)] = E_ERR - P_ERR\n",
    "    \n",
    "    # add ERR delta measures to dict\n",
    "    delta_values[(P,E)].append(E_ERR -  P_ERR)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and interleaving pairs\n",
    "In the following cells the pairs that hold delta values that are positive are extracted for further use. We extracted the pairs holding a positive delta value for all offline measures. Moreover, we took the mean of delta values from all offline measures and extracted the pairs that had a positive delta value for this mean.\n",
    "Afterwards, we used balance interleaving to create one ranking out of both algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Filter pairs #####\n",
    "\n",
    "def filter_all_pairs(pairs_dict):\n",
    "    # this function is used to extract pairs that have a positive delta value for the mean of all offline measures\n",
    "    pairs_list = []\n",
    "    dict_items = pairs_dict.items()\n",
    "    for pair in dict_items:\n",
    "        scores = pair[1]\n",
    "        avg = sum(scores, 0.0)/len(scores)\n",
    "        if avg > 0.0:\n",
    "            pairs_list.append(pair[0])\n",
    "    \n",
    "    return pairs_list\n",
    "\n",
    "def filter_metric_pairs(pairs_dict):\n",
    "    # this function is used to extract pairs that have a positive delta value based on offline measures\n",
    "    pairs_list = []\n",
    "    dict_items = pairs_dict.items()\n",
    "    for pair in dict_items:\n",
    "        scores = pair[1]\n",
    "        if scores > 0.0:\n",
    "            pairs_list.append(pair[0])\n",
    "    \n",
    "    return pairs_list\n",
    "\n",
    "\n",
    "# Iterate through pairs\n",
    "def interleaving(pairs):\n",
    "    all_results = []\n",
    "    all_origins = []\n",
    "    for pair in pairs:\n",
    "\n",
    "        # Flip a coin, assign winning and losing\n",
    "        # P = pair[0], E = pair[1]\n",
    "        coin_winner = random.randint(0,1)\n",
    "        winner = pair[coin_winner]\n",
    "        loser = pair[1 - coin_winner]\n",
    "\n",
    "        # initiate lists\n",
    "        resulting_list = []\n",
    "        origin_list = []\n",
    "\n",
    "        # iterate through lists, fill up results and origin list\n",
    "        for i in range(len(winner)):\n",
    "            resulting_list.append(winner[i])\n",
    "            origin_list.append(coin_winner)\n",
    "            resulting_list.append(loser[i])\n",
    "            origin_list.append(1-coin_winner)\n",
    "\n",
    "        all_results.append(resulting_list)\n",
    "        all_origins.append(origin_list)\n",
    "    \n",
    "    return all_results, all_origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Balanced Interleaving #####\n",
    "\n",
    "print(\"Before:\", len(delta_values.items()))\n",
    "\n",
    "all_pairs = filter_all_pairs(delta_values)\n",
    "\n",
    "print(\"After:\", len(all_pairs))\n",
    "\n",
    "all_results, all_origins = interleaving(all_pairs)\n",
    "\n",
    "# now get pairs for which delta measure is positive for every offline metric\n",
    "AP_pairs = filter_metric_pairs(AP_delta)\n",
    "DCG_pairs = filter_metric_pairs(nDCG_delta)\n",
    "ERR_pairs = filter_metric_pairs(ERR_delta)\n",
    "\n",
    "# interleave the pairs for which E outperforms P per offline metric\n",
    "AP_results, AP_origins = interleaving(AP_pairs)\n",
    "DCG_results, DCG_origins = interleaving(DCG_pairs)\n",
    "ERR_results, ERR_origins = interleaving(ERR_pairs)\n",
    "\n",
    "print(len(all_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Click Model\n",
    "The following cells represent the random click model and its simulation. Simulation is run for A) every offline measure for which E outperforms P and B) the mean delta values of all offline measures for which E outperforms P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Random Click Model #####\n",
    "\n",
    "# Learns parameter\n",
    "def learn_param_RCM(data):\n",
    "    \n",
    "    # open file and read\n",
    "    lines=data.readlines()\n",
    "\n",
    "    clicks = 0\n",
    "    documents = 0\n",
    "\n",
    "    # Acquire total amount of queries and clicks\n",
    "    for line in lines:\n",
    "        items = re.split(r'\\t+',line)\n",
    "        if items[2] == \"Q\":\n",
    "            # Per query 10 documents are shown\n",
    "            documents += 10\n",
    "        elif items[2] == \"C\":\n",
    "            clicks += 1\n",
    "    \n",
    "    # Calculate rho\n",
    "    rho = clicks/documents\n",
    "    \n",
    "    return rho\n",
    "\n",
    "# Predicts a click probability\n",
    "def predict_prob_RCM(ranking, param):\n",
    "    \n",
    "    # get the click probability for every document in ranking\n",
    "    click_prob = []\n",
    "    for doc in ranking:\n",
    "        click_prob.append(param)\n",
    "        \n",
    "    return click_prob\n",
    "\n",
    "# Decide whether document is clicked on\n",
    "def click_doc_RCM(click_prob):\n",
    "    clicked = []\n",
    "    for prob in click_prob:\n",
    "        chance = random.random()\n",
    "        if chance <= prob:\n",
    "            clicked.append(1)\n",
    "        else:\n",
    "            clicked.append(0)\n",
    "    return clicked\n",
    "\n",
    "def RCM_simulation(pairs, origins, rho):\n",
    "\n",
    "    N = 50\n",
    "    p_RCM_list = []\n",
    "\n",
    "    # simulate experiment N times\n",
    "    for i in range(N):\n",
    "        # keep track of which algorithm won\n",
    "        E_win = 0\n",
    "        P_win = 0\n",
    "        # loop through all rankings\n",
    "        for j, ranking in enumerate(pairs):\n",
    "\n",
    "            # predict probability of clicking\n",
    "            click_prob = predict_prob_RCM(ranking, rho)\n",
    "\n",
    "            # get which documents were clicked\n",
    "            clicked = click_doc_RCM(click_prob)\n",
    "\n",
    "            # now shuffle the origin list so documents are picked at random\n",
    "            origin_shuffle = random.sample(origins[j], len(origins[j]))\n",
    "\n",
    "            E_click = 0\n",
    "            P_click = 0\n",
    "            # check whether the clicked documents were produced by E or P\n",
    "            for h, click in enumerate(clicked):\n",
    "                if click == 1 and origin_shuffle[h] == 1:\n",
    "                    E_click += 1\n",
    "                elif click == 1 and origin_shuffle[h] == 0:\n",
    "                    P_click += 1\n",
    "\n",
    "            # determine whether E or P won\n",
    "            if E_click > P_click:\n",
    "                E_win += 1\n",
    "            elif P_click > E_click:\n",
    "                P_win += 1\n",
    "\n",
    "        # proportion of times E won\n",
    "        p = E_win / (E_win + P_win)\n",
    "        p_RCM_list.append(p)\n",
    "        \n",
    "    return p_RCM_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Simulate random click model ######\n",
    "\n",
    "# get parameter out of data\n",
    "f=open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "rho = learn_param_RCM(f)\n",
    "f.close()\n",
    "\n",
    "# get the p for the average of all metrics\n",
    "p_RCM_list = RCM_simulation(all_results, all_origins, rho)\n",
    "\n",
    "# get the p for every metric \n",
    "p_AP_RCM_list = RCM_simulation(AP_results, AP_origins, rho)\n",
    "p_DCG_RCM_list = RCM_simulation(DCG_results, DCG_origins, rho)\n",
    "p_ERR_RCM_list = RCM_simulation(ERR_results, ERR_origins, rho)\n",
    "\n",
    "print(p_RCM_list)\n",
    "print(p_AP_RCM_list)\n",
    "print(p_DCG_RCM_list)\n",
    "print(p_ERR_RCM_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Dynamic Bayesian Model\n",
    "The following cells represent the simiple dynamic bayesian model and its simulation. Simulation is run for A) every offline measure for which E outperforms P and B) the mean delta values of all offline measures for which E outperforms P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Simple Dynamic Bayesian Model #####\n",
    "\n",
    "# Learns parameter\n",
    "def learn_param_DBM(file):\n",
    "    \n",
    "    lines = file.readlines()\n",
    "\n",
    "    #previous_session = 0 # Keep track of session number to determine if click is last click.\n",
    "    previous_type = \"\"\n",
    "    \n",
    "    clicks = 0\n",
    "    #last_clicks_session = 0\n",
    "    last_clicks_query = 0\n",
    "\n",
    "    lines.reverse() # Reversed order, so it is detectable if a click is last.\n",
    "    for line in lines:\n",
    "        items = re.split(r'\\t+',line) #strip tabs\n",
    "        #current_session = items[0]\n",
    "        current_type = items[2]\n",
    "        #if current_type == \"C\" and current_session != previous_session:\n",
    "            #last_clicks_session += 1\n",
    "        if current_type == \"C\" and previous_type == \"Q\": \n",
    "            last_clicks_query += 1\n",
    "        if current_type == \"C\":\n",
    "            clicks += 1\n",
    "        #previous_session = current_session\n",
    "        previous_type = current_type\n",
    "\n",
    "    sigma = last_clicks_query/clicks\n",
    "    \n",
    "    return sigma\n",
    "        \n",
    "\n",
    "# Predicts a click probability\n",
    "def predict_prob_DBM(rank, sigma):\n",
    "    # for the click probability, we'll need P(A) and P(E)\n",
    "\n",
    "    # first get alpha, which will be set according to the level of relevance of a document\n",
    "    if rank == 'HR':\n",
    "        alpha = 0.9\n",
    "    elif rank == 'R':\n",
    "        alpha = 0.3\n",
    "    elif rank == 'N':\n",
    "        alpha = 0 \n",
    "    \n",
    "    # check if user will click on the document (depending on alpha)\n",
    "    x = random.random()\n",
    "    if x <= alpha:\n",
    "        P_A = 1\n",
    "    else:\n",
    "        P_A = 0\n",
    "            \n",
    "    # since we are using a simple DBM, gamma will always be one    \n",
    "    gamma = 1\n",
    "    \n",
    "    return P_A, gamma  \n",
    "       \n",
    "        \n",
    "# Decide which documents are clicked\n",
    "def click_doc_DBM(ranking, sigma):\n",
    "    # this function takes a ranking list and a value for the parameter sigma as input and uses\n",
    "    # these to determine which documents in the ranking list are clicked on\n",
    "    \n",
    "    # set P(E) to 1 (first snippet is always read)\n",
    "    P_E = 1\n",
    "    \n",
    "    clicked = []\n",
    "    \n",
    "    # run through the ranking to decide whether a document will be clicked or not\n",
    "    for rank in ranking:\n",
    "        P_A, gamma = predict_prob_DBM(rank, sigma)\n",
    "        \n",
    "        # based on probability, set click to 1 or 0\n",
    "        if P_A == 1 and P_E == 1:\n",
    "            P_C = 1\n",
    "        else:\n",
    "            P_C = 0\n",
    "     \n",
    "        clicked.append(P_C)\n",
    "        \n",
    "        # if user has clicked, check if user is satisfied\n",
    "        if P_C == 1:\n",
    "            # now check if user is satisfied\n",
    "            x = random.random()\n",
    "            if x <= sigma:\n",
    "                # if satisfied, user will not read any more snippets (thus click nothing)\n",
    "                P_E = 0\n",
    "            else:\n",
    "                # if user is not satisfied, user will read next snippet (thus possibly click)\n",
    "                P_E = 1 \n",
    "        \n",
    "    return clicked       \n",
    "\n",
    "def DBM_simulation(pairs, origins, sigma):\n",
    "    N = 50\n",
    "    p_DBM_list = []\n",
    "    # simulate experiment N times\n",
    "    for i in range(N):\n",
    "\n",
    "        # keep track of which algorithm won\n",
    "        E_win = 0\n",
    "        P_win = 0\n",
    "        # loop through all rankings\n",
    "        for j, ranking in enumerate(pairs):\n",
    "\n",
    "            # get which documents were clicked\n",
    "            clicked = click_doc_DBM(ranking, sigma)\n",
    "\n",
    "            E_click = 0\n",
    "            P_click = 0\n",
    "\n",
    "            current_origin = origins[j]\n",
    "\n",
    "            # check whether the clicked documents were produced by E or P\n",
    "            for h, click in enumerate(clicked):\n",
    "                if click == 1 and current_origin[h] == 1:\n",
    "                    E_click += 1\n",
    "                elif click == 1 and current_origin[h] == 0:\n",
    "                    P_click += 1\n",
    "\n",
    "            # determine whether E or P won\n",
    "            if E_click > P_click:\n",
    "                E_win += 1\n",
    "            elif P_click > E_click:\n",
    "                P_win += 1\n",
    "        #print(E_win, P_win)\n",
    "        # proportion of times E won\n",
    "        p = E_win / (E_win + P_win)\n",
    "        p_DBM_list.append(p)\n",
    "    return p_DBM_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Simulate dynamic bayesian model #####\n",
    "\n",
    "# get parameter out of data\n",
    "f=open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "sigma = learn_param_DBM(f)\n",
    "f.close()\n",
    "\n",
    "print(len(all_results))\n",
    "# get the p for the average of all metrics\n",
    "p_DBM_list = DBM_simulation(all_results, all_origins, sigma)\n",
    "\n",
    "\n",
    "# get the p for every metric \n",
    "p_AP_DBM_list = DBM_simulation(AP_results, AP_origins, sigma)\n",
    "p_DCG_DBM_list = DBM_simulation(DCG_results, DCG_origins, sigma)\n",
    "p_ERR_DBM_list = DBM_simulation(ERR_results, ERR_origins, sigma)\n",
    "\n",
    "print(p_DBM_list)\n",
    "print(p_AP_DBM_list)\n",
    "print(p_DCG_DBM_list)\n",
    "print(p_ERR_DBM_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Effect of different values for delta on \n",
    "# create new dict with all values for which delta is positive\n",
    "pairs_dict = {}\n",
    "for key, value in delta_values.items():\n",
    "    avg = sum(value, 0.0)/len(value)\n",
    "    if avg > 0.0:\n",
    "        pairs_dict[key] = avg\n",
    "\n",
    "keys = sorted(pairs_dict, key=pairs_dict.get)\n",
    "\n",
    "index = round(len(pairs_dict)/10)\n",
    "# take first 10 % of the sorted dictionary to represent the low delta values\n",
    "low_deltas = keys[:index]\n",
    "\n",
    "# take last 10 % of the sorted dictionary to represent the high delta values\n",
    "high_deltas = keys[-index:]\n",
    "\n",
    "# interleave both pair lists\n",
    "low_delta_results, low_delta_origins = interleaving(low_deltas)\n",
    "high_delta_results, high_delta_origins = interleaving(high_deltas)\n",
    "\n",
    "# get rho out of data\n",
    "f=open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "rho = learn_param_RCM(f)\n",
    "f.close()\n",
    "\n",
    "# get sigma out of data                     \n",
    "f=open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "sigma = learn_param_DBM(f)\n",
    "f.close()\n",
    "\n",
    "# run the random click model on both delta's\n",
    "p_RCM_ld = RCM_simulation(low_delta_results, low_delta_origins, rho)\n",
    "p_RCM_hd = RCM_simulation(high_delta_results, high_delta_origins, rho)\n",
    "\n",
    "# run the dynamic bayesion model on both delta's\n",
    "p_DBM_ld = DBM_simulation(low_delta_results, low_delta_origins, sigma)\n",
    "p_DBM_hd = DBM_simulation(high_delta_results, high_delta_origins, sigma)\n",
    "\n",
    "# run statistical tests to find significant differences\n",
    "scipy.stats.ttest_ind(p_DBM_ld, p_DBM_hd)\n",
    "\n",
    "####### CREATE PLOT ########\n",
    "\n",
    "multiple_bars = plt.figure()\n",
    "\n",
    "x = [0.25,0.8]\n",
    "names = ['RCM', 'DBM']\n",
    "\n",
    "bars_RCM = [sum(p_RCM_ld)/len(p_RCM_ld), sum(p_DBM_ld)/len(p_DBM_ld)]\n",
    "bars_DBM = [sum(p_RCM_hd)/len(p_RCM_hd), sum(p_DBM_hd)/len(p_DBM_hd)]\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.bar(x[0]-0.1, bars_RCM[0] ,width=0.2,color='b',align='center')\n",
    "ax.bar(x[0]+0.1, bars_RCM[1] ,width=0.2,color='g',align='center')\n",
    "\n",
    "ax.bar(x[1]-0.1, bars_DBM[0] ,width=0.2,color='b',align='center')\n",
    "ax.bar(x[1]+0.1, bars_DBM[1] ,width=0.2,color='g',align='center')\n",
    "ax.set_xticks([x[0],x[1]])\n",
    "ax.set_xticklabels(names)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### PLOT OVER THE AVERAGE OF ALL METRICS #####\n",
    "\n",
    "# average over all metrics and standard deviation for random click model\n",
    "avg_ALL_RCM = statistics.mean(p_RCM_list)\n",
    "stdev_ALL_RCM = statistics.pstdev(p_RCM_list)\n",
    "\n",
    "# average over all metrics and standard deviation for dynamic bayesian model\n",
    "avg_ALL_DBM = statistics.mean(p_DBM_list)\n",
    "stdev_ALL_DBM = statistics.pstdev(p_DBM_list)\n",
    "\n",
    "# plot bar graph over all metrics, RCM compared to DBM\n",
    "ax = plt.gca()\n",
    "\n",
    "x = range(1, 3)\n",
    "y = (avg_ALL_RCM, avg_ALL_DBM)\n",
    "z = (stdev_ALL_RCM, stdev_ALL_DBM)\n",
    "names = ('RCM', 'DBM')\n",
    "clr = ('r', 'b')\n",
    "plt.bar(x, y, width=0.5, color = clr, yerr = z, align=\"center\")\n",
    "\n",
    "ind = range(1, 3)\n",
    "plt.xticks(ind, x)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### PLOTS FOR EACH METRIC #####\n",
    "\n",
    "# average and standard deviation per metric for random click model\n",
    "avg_AP_RCM = statistics.mean(p_AP_RCM_list)\n",
    "stdev_AP_RCM = statistics.pstdev(p_AP_RCM_list)\n",
    "\n",
    "avg_DCG_RCM = statistics.mean(p_DCG_RCM_list)\n",
    "stdev_DCG_RCM = statistics.pstdev(p_DCG_RCM_list)\n",
    "\n",
    "avg_ERR_RCM = statistics.mean(p_ERR_RCM_list)\n",
    "stdev_ERR_RCM = statistics.pstdev(p_ERR_RCM_list)\n",
    "\n",
    "# average and standard deviation per metric for random click model\n",
    "avg_AP_DBM = statistics.mean(p_AP_DBM_list)\n",
    "stdev_AP_DBM = statistics.pstdev(p_AP_DBM_list)\n",
    "\n",
    "avg_DCG_DBM = statistics.mean(p_DCG_DBM_list)\n",
    "stdev_DCG_DBM = statistics.pstdev(p_DCG_DBM_list)\n",
    "\n",
    "avg_ERR_DBM = statistics.mean(p_ERR_DBM_list)\n",
    "stdev_ERR_DBM = statistics.pstdev(p_ERR_DBM_list)\n",
    "\n",
    "\n",
    "# plot bar graph over all metrics, RCM compared to DBM\n",
    "ax = plt.gca()\n",
    "\n",
    "x = range(1, 7)\n",
    "y = (avg_AP_RCM, avg_DCG_RCM, avg_ERR_RCM, avg_AP_DBM, avg_DCG_DBM, avg_ERR_DBM)\n",
    "z = (stdev_AP_RCM, stdev_DCG_RCM, stdev_ERR_RCM, stdev_AP_DBM, stdev_DCG_DBM, stdev_ERR_DBM)\n",
    "names = ('AP', 'DCG', 'ERR', 'AP', 'DCG', 'ERR')\n",
    "clr = ('r', 'r', 'r', 'b', 'b', 'b')\n",
    "plt.bar(x, y, width=0.5, color = clr, yerr = z, align=\"center\")\n",
    "\n",
    "ind = range(1, 7)\n",
    "plt.xticks(ind, x)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Check percentages of overlapping pairs between offline measures #####,\n",
    "\n",
    "print(\"AP:\",len(AP_pairs))\n",
    "print(\"DCG:\",len(DCG_pairs))\n",
    "print(\"ERR:\",len(ERR_pairs))\n",
    "\n",
    "print(\"AP+DCG:\", (1- (len(set(AP_pairs+DCG_pairs))/(len(AP_pairs)+ len(DCG_pairs))))*2)\n",
    "print(\"AP+ERR:\", (1- (len(set(AP_pairs+ERR_pairs))/(len(AP_pairs)+ len(ERR_pairs))))*2)\n",
    "print(\"DCG+ERR:\", (1- (len(set(DCG_pairs+ERR_pairs))/(len(DCG_pairs)+ len(ERR_pairs))))*2)\n",
    "print(\"AP+DCG+ERR:\", (1- (len(set(AP_pairs+DCG_pairs+ERR_pairs))/(len(AP_pairs)+len(DCG_pairs)+len(ERR_pairs))))*2)\n",
    "\n",
    "print(\"AP in DCG:\", len(list(set(AP_pairs).intersection(DCG_pairs)))/len(AP_pairs))\n",
    "print(\"AP in ERR:\", len(list(set(AP_pairs).intersection(ERR_pairs)))/len(AP_pairs))\n",
    "print(\"DCG in AP:\", len(list(set(DCG_pairs).intersection(AP_pairs)))/len(DCG_pairs))\n",
    "print(\"DCG in ERR:\", len(list(set(DCG_pairs).intersection(ERR_pairs)))/len(DCG_pairs))\n",
    "print(\"ERR in AP:\", len(list(set(ERR_pairs).intersection(AP_pairs)))/len(ERR_pairs))\n",
    "print(\"ERR in DCG:\", len(list(set(ERR_pairs).intersection(DCG_pairs)))/len(ERR_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
