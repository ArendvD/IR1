{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "## by Oscar Ligthart, Nicole Ferreira Silverio and Arend van Dormalen.\n",
    "\n",
    "Student numbers:\n",
    "\n",
    "Oscar Ligthart:            10557520\n",
    "\n",
    "Nicole Ferreira Silverio : 10521933\n",
    "\n",
    "Arend van Dormalen:        10615199\n",
    "\n",
    "\n",
    "This Jupyter Notebook contains the first project for Information Retrieval 1 taught at the UvA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1A\n",
    "\n",
    "The chance of a type 1 error ($\\alpha$) increases each time an experiment is repeated, if it's not corrected. The new $\\alpha$ for _m_ experiments is $1 − (1 − \\alpha)^m ≈ m\\alpha$. This is called the Family-wise error rate.\n",
    "\n",
    "# Question 1B,\n",
    "\n",
    "The chance of a type 1 error needs to be corrected in order to find the chance of an experiment existing that rejects the null hypothesis. This can be done by the Bonferroni correction, which rejects the null hypothesis for any $p_i$ when it it lower than or equal to $\\frac{\\alpha}{m}$.\n",
    "      \n",
    "\n",
    "# Question 2\n",
    "\n",
    "Assume two ranked lists created by two different rankers. List $l1$ contains documents $d1$, $d2$ and $d3$ in that order. List $l2$ contains documents $d2$, $d3$ and $d4$ in that order. Now assume that the only relevant document is $d3$, which will therefore be clicked on most often. From our judgment, it is obvious that $l2$ is the most relevant list as it has placed $d3$ on a higher position. However, in Team Draft Interleaving, these algorithms will be evaluated as having equal performance.\n",
    "\n",
    "In this situation, $d3$ will always be the third item on the interleaved list. After the first coin flip, $d2$ will be removed from $l1$ as this document has already been supplied by $l2$.  At the second coin flip, $d3$ will be the next document for both lists. This causes the relevance for both lists to be the same, as they now both have the same chance of supplying the only relevant document to the interleaved list.\n",
    "\n",
    "The same problem occurs when list $l1$ contains documents $d1$, $d2$, and $d3$ and list $l2$ contains documents $d2$, $d3$, and $d1$. If $d1$ and $d2$ are relevant in $48\\%$ of the time and $d3$ in $4\\%$ of the time, it will lead to a identical performance, while $l1$ is superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Part\n",
    "\n",
    "The following cell represents the creation of all possible relevance pairs E and P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import scipy.stats\n",
    "\n",
    "# first get the sequence options\n",
    "relevance = ['N', 'R', 'HR']\n",
    "options = list(itertools.product(relevance, repeat = 5))\n",
    "\n",
    "# create all possible pairs in sequence options\n",
    "pair_index = list(itertools.permutations(range(len(options)), 2))\n",
    "\n",
    "\n",
    "pairs = []\n",
    "\n",
    "# create nested list holding all possible pairs\n",
    "for temp_pair in pair_index:\n",
    "    pairs.append([options[temp_pair[0]], options[temp_pair[1]]])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline measures\n",
    "\n",
    "Each cell represents one of the following three offline measures: Average precision, nDCG, ERR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_precision(ranking):\n",
    "    \n",
    "    ''' This function calculates the average precision of a ranking of documents\n",
    "    it takes a ranking as input and returns the amount of relative documents\n",
    "    and the value that will represent the numerator of the equation\n",
    "    '''\n",
    "    \n",
    "    rel = 0\n",
    "    AP_numerator = 0\n",
    "    # get amount of relevant documents\n",
    "    for i, doc in enumerate(ranking):\n",
    "        if doc == 'R' or doc == 'HR':\n",
    "            rel += 1\n",
    "            AP_numerator += rel/(i+1)\n",
    "            \n",
    "    return rel, AP_numerator\n",
    "\n",
    "# new dict for average precision for both P and E (key is pair, value is average precisions))\n",
    "AP_delta = {}\n",
    "\n",
    "# new dict for delta measures (AP, nDCG and ERR will be stored here per pair)\n",
    "delta_values = {}\n",
    "\n",
    "# get precision for all pairs\n",
    "for pair in pairs:\n",
    "    # first calculate numerator for average precision for P\n",
    "    P = pair[0]    \n",
    "    P_rel, P_AP_numerator = get_average_precision(P)\n",
    " \n",
    "    # now calculate numerator for average precision for E\n",
    "    E = pair[1]\n",
    "    E_rel, E_AP_numerator = get_average_precision(E)\n",
    "\n",
    "    # get total number of relevant documents returned from query\n",
    "    total_rel = P_rel + E_rel\n",
    "    \n",
    "    # calculate average precision for both P and E\n",
    "    P_AP = P_AP_numerator/total_rel\n",
    "    E_AP = E_AP_numerator/total_rel\n",
    "    \n",
    "    # store results in a dict\n",
    "    AP_delta[(P,E)] = [E_AP - P_AP]\n",
    "    \n",
    "    # store AP delta measures in list format in dict\n",
    "    delta_values[(P,E)] = [E_AP - P_AP]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nDCG(ranking):\n",
    "    \n",
    "    ''' This function calculates the normaliced discounted cumulative gain (nDCG)\n",
    "    it takes a ranking of documents as input and outputs the nDCG measure value of that ranking\n",
    "    '''\n",
    "    \n",
    "    DCG = 0\n",
    "    \n",
    "    # loop through ranking\n",
    "    for i, rank in enumerate(ranking):\n",
    "        # decide what the relative rank is\n",
    "        if rank == 'HR':\n",
    "            rel_r = 2\n",
    "        elif rank == 'R':\n",
    "            rel_r = 1\n",
    "        elif rank == 'N':\n",
    "            rel_r = 0\n",
    "        \n",
    "        DCG += (2**rel_r - 1)/(np.log2(1+(i+1)))\n",
    "    \n",
    "    # normalize\n",
    "    DCG = DCG/len(ranking)\n",
    "    \n",
    "    return DCG\n",
    "\n",
    "# new dict for average precision for both P and E (key is pair, value is average precisions))\n",
    "nDCG_delta = {}\n",
    "\n",
    "counter = 0\n",
    "same_counter = 0\n",
    "lower_counter = 0\n",
    "\n",
    "# get nDCG for all pairs\n",
    "for pair in pairs:    \n",
    "    # first for P\n",
    "    P = pair[0]\n",
    "    P_DCG = get_nDCG(P)\n",
    "    \n",
    "    # then for E\n",
    "    E = pair[1]\n",
    "    E_DCG = get_nDCG(E)\n",
    "    \n",
    "    nDCG_delta[(P,E)] = [E_DCG - P_DCG]\n",
    "    \n",
    "    # add nDCG delta measure to dict\n",
    "    delta_values[(P,E)].append(E_DCG - P_DCG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ERR(ranking):\n",
    "    \n",
    "    ''' This function calculates the expected reciprocal rank\n",
    "    it takes a ranking of documents as input and returns the ERR measure value of that rank\n",
    "    this code is based on the paper on ERR\n",
    "    '''\n",
    "    \n",
    "    # initialize variables\n",
    "    ERR = 0\n",
    "    p = 1\n",
    "    max_rel = 2\n",
    "    \n",
    "    # loop through ranking\n",
    "    for i, rank in enumerate(ranking):        \n",
    "        # start at second rank\n",
    "        if i != 0:            \n",
    "            # decide what the relative rank is\n",
    "            if rank == 'HR':\n",
    "                rel_r = 2\n",
    "            elif rank == 'R':\n",
    "                rel_r = 1\n",
    "            elif rank == 'N':\n",
    "                rel_r = 0\n",
    "\n",
    "            # Calculate R with the mapping function\n",
    "            R = (2**rel_r - 1)/(2**max_rel)\n",
    "\n",
    "            # Modify ERR value\n",
    "            ERR += p * (R/i)\n",
    "\n",
    "            # Modify p\n",
    "            p = p*(1-R)\n",
    "    \n",
    "    return ERR\n",
    "\n",
    "# new dict for ERR values for both P and E (key is pair, value is ERR value))\n",
    "ERR_delta = {}\n",
    "\n",
    "# get ERR for all pairs\n",
    "for pair in pairs:\n",
    "    \n",
    "    # first for P\n",
    "    P = pair[0]\n",
    "    P_ERR = get_ERR(P)\n",
    "    \n",
    "    # then for E\n",
    "    E = pair[1]\n",
    "    E_ERR = get_ERR(E)\n",
    "    \n",
    "    # store delta values in dict\n",
    "    ERR_delta[(P,E)] = [E_ERR - P_ERR]\n",
    "    \n",
    "    # add ERR delta measures to dict\n",
    "    delta_values[(P,E)].append(E_ERR -  P_ERR)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and interleaving pairs\n",
    "In the following cells the pairs that hold delta values that are positive are extracted for further use. We extracted the pairs holding a positive delta value for all offline measures. Moreover, we took the mean of delta values from all offline measures and extracted the pairs that had a positive delta value for this mean.\n",
    "Afterwards, we used Balanced Interleaving to create one ranking out of both algorithms for every pair where E outperforms P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Filter pairs #####\n",
    "\n",
    "def filter_pairs(pairs_dict):\n",
    "    ''' This function is used to extract pairs that have a positive delta value for the mean of all offline measures\n",
    "    it takes a dictionary of pairs as input and returns a list of pairs from that dictionary that hold \n",
    "    a positive value for the delta measure\n",
    "    '''\n",
    "    \n",
    "    pairs_list = []\n",
    "    dict_items = pairs_dict.items()\n",
    "    \n",
    "    # loop through dictionary\n",
    "    for pair in dict_items:\n",
    "        # extract delta value\n",
    "        scores = pair[1]\n",
    "        \n",
    "        # append delta value if it is above 0\n",
    "        avg = sum(scores, 0.0)/len(scores)\n",
    "        if avg > 0.0:\n",
    "            pairs_list.append(pair[0])\n",
    "    \n",
    "    return pairs_list\n",
    "\n",
    "def interleaving(pairs):\n",
    "    ''' This function uses balanced interleaving to interleave a pair into one ranking\n",
    "    it takes a list holding all pairs as input (where a pair represents E and P)\n",
    "    and outputs a list holding the resulting ranking and a list holding the origins\n",
    "    of those elements (whether the document came from E or P)\n",
    "    '''\n",
    "    \n",
    "    # initialize output lists\n",
    "    all_results = []\n",
    "    all_origins = []\n",
    "    \n",
    "    # for all pairs\n",
    "    for pair in pairs:\n",
    "\n",
    "        # Flip a coin, assign winning and losing\n",
    "        # P = pair[0], E = pair[1]\n",
    "        coin_winner = random.randint(0,1)\n",
    "        winner = pair[coin_winner]\n",
    "        loser = pair[1 - coin_winner]\n",
    "\n",
    "        # initiate lists\n",
    "        resulting_list = []\n",
    "        origin_list = []\n",
    "\n",
    "        # iterate through lists, fill up results and origin list\n",
    "        for i in range(len(winner)):\n",
    "            resulting_list.append(winner[i])\n",
    "            origin_list.append(coin_winner)\n",
    "            resulting_list.append(loser[i])\n",
    "            origin_list.append(1-coin_winner)\n",
    "\n",
    "        all_results.append(resulting_list)\n",
    "        all_origins.append(origin_list)\n",
    "    \n",
    "    return all_results, all_origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Balanced Interleaving #####\n",
    "\n",
    "# filter and interleave pairs based on the average delta value over all offline metrics\n",
    "all_pairs = filter_pairs(delta_values)\n",
    "all_results, all_origins = interleaving(all_pairs)\n",
    "\n",
    "# now get pairs for which delta measure is positive for every offline metric\n",
    "AP_pairs = filter_pairs(AP_delta)\n",
    "DCG_pairs = filter_pairs(nDCG_delta)\n",
    "ERR_pairs = filter_pairs(ERR_delta)\n",
    "\n",
    "# interleave the pairs for which E outperforms P per offline metric\n",
    "AP_results, AP_origins = interleaving(AP_pairs)\n",
    "DCG_results, DCG_origins = interleaving(DCG_pairs)\n",
    "ERR_results, ERR_origins = interleaving(ERR_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Click Model\n",
    "The following cells represent the Random Click Model (RCM) and its simulation. Simulation is run for A) every offline measure for which E outperforms P and B) the mean delta values of all offline measures for which E outperforms P.\n",
    "In this model, all queries and documents are assumed to be unique. There was a very small number of queries and documents of which the ID appeared more than once. Since we can only know the ID of a query or document, but not the content, it will be very difficult to calculate the similarity of queries or documents. Given the low occurence of duplicates it is better to treat them as unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Random Click Model #####\n",
    "\n",
    "def learn_param_RCM(data):\n",
    "    ''' This function learns the probability of the event of a click based\n",
    "    on a click log \n",
    "    It takes training data as input and returns rho (click probability)\n",
    "    '''\n",
    "    \n",
    "    # open file and read\n",
    "    lines=data.readlines()\n",
    "\n",
    "    clicks = 0\n",
    "    documents = 0\n",
    "\n",
    "    # Acquire total amount of queries and clicks\n",
    "    for line in lines:\n",
    "        items = re.split(r'\\t+',line)\n",
    "        if items[2] == \"Q\":\n",
    "            # Per query 10 documents are shown\n",
    "            documents += 10\n",
    "        elif items[2] == \"C\":\n",
    "            clicks += 1\n",
    "    \n",
    "    # Calculate rho\n",
    "    rho = clicks/documents\n",
    "    \n",
    "    return rho\n",
    "\n",
    "\n",
    "def predict_prob_RCM(ranking, param):\n",
    "    ''' Predicts a click probability '''\n",
    "    \n",
    "    # get the click probability for every document in ranking\n",
    "    click_prob = []\n",
    "    for doc in ranking:\n",
    "        click_prob.append(param)\n",
    "        \n",
    "    return click_prob\n",
    "\n",
    "def click_doc_RCM(click_prob):\n",
    "    '''This function uses the click probability to determine whether a document is clicked on or not '''\n",
    "    \n",
    "    clicked = []\n",
    "    for prob in click_prob:\n",
    "        chance = random.random()\n",
    "        if chance <= prob:\n",
    "            clicked.append(1)\n",
    "        else:\n",
    "            clicked.append(0)\n",
    "    return clicked\n",
    "\n",
    "\n",
    "def RCM_simulation(pairs, origins, rho, N):\n",
    "    '''The following function represents the simulation of the random click model\n",
    "    \n",
    "    the function takes an interleaved ranking and a list which tells which element\n",
    "    in the ranking comes from which algorithm. Furthermore, parameters rho and N\n",
    "    are given as input, representing click probability and number of simulations \n",
    "    respectively.\n",
    "    \n",
    "    output consists of a list consisting of N proportions where E\n",
    "    outperforms P for all pairs\n",
    "    '''\n",
    "    \n",
    "    # initialize a list holding the proportion of wins for E\n",
    "    p_RCM_list = []\n",
    "    list_wins = []\n",
    "    list_trials = []\n",
    "\n",
    "    # simulate experiment N times\n",
    "    for i in range(N):\n",
    "        # keep track of which algorithm won\n",
    "        E_win = 0\n",
    "        P_win = 0\n",
    "        \n",
    "        # loop through all rankings\n",
    "        for j, ranking in enumerate(pairs):\n",
    "\n",
    "            # predict probability of clicking\n",
    "            click_prob = predict_prob_RCM(ranking, rho)\n",
    "\n",
    "            # get which documents were clicked\n",
    "            clicked = click_doc_RCM(click_prob)\n",
    "\n",
    "            # now shuffle the origin list so documents are picked at random\n",
    "            origin_shuffle = random.sample(origins[j], len(origins[j]))\n",
    "\n",
    "            E_click = 0\n",
    "            P_click = 0\n",
    "            # check whether the clicked documents were produced by E or P\n",
    "            for h, click in enumerate(clicked):\n",
    "                if click == 1 and origin_shuffle[h] == 1:\n",
    "                    E_click += 1\n",
    "                elif click == 1 and origin_shuffle[h] == 0:\n",
    "                    P_click += 1\n",
    "\n",
    "            # determine whether E or P won\n",
    "            if E_click > P_click:\n",
    "                E_win += 1\n",
    "            elif P_click > E_click:\n",
    "                P_win += 1\n",
    "\n",
    "        # proportion of times E won\n",
    "        if (E_win - P_win) == 0:\n",
    "            p = 0.5\n",
    "        else:\n",
    "            p = E_win / (E_win + P_win)\n",
    "        p_RCM_list.append(p)\n",
    "        \n",
    "        list_wins.append(E_win)\n",
    "        list_trials.append((E_win + P_win))\n",
    "        \n",
    "    return p_RCM_list, list_wins, list_trials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Simulate random click model ######\n",
    "\n",
    "# get parameter out of data\n",
    "f=open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "rho = learn_param_RCM(f)\n",
    "f.close()\n",
    "\n",
    "# Run N simulations\n",
    "N = 50\n",
    "\n",
    "# get the p for the average of all metrics\n",
    "p_RCM_list, RCM_ALL_wins, RCM_ALL_trials = RCM_simulation(all_results, all_origins, rho, N)\n",
    "\n",
    "# get the p for every metric \n",
    "p_AP_RCM_list, RCM_AP_wins, RCM_AP_trials = RCM_simulation(AP_results, AP_origins, rho, N)\n",
    "p_DCG_RCM_list, RCM_DCG_wins, RCM_DCG_trials = RCM_simulation(DCG_results, DCG_origins, rho, N)\n",
    "p_ERR_RCM_list, RCM_ERR_wins, RCM_ERR_trials = RCM_simulation(ERR_results, ERR_origins, rho, N)\n",
    "\n",
    "print(p_RCM_list)\n",
    "print(p_AP_RCM_list)\n",
    "print(p_DCG_RCM_list)\n",
    "print(p_ERR_RCM_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Dynamic Bayesian Model\n",
    "The following cells represent the simple Dynamic Bayesian Model (DBM) and its simulation. Simulation is run for A) every offline measure for which E outperforms P and B) the mean delta values of all offline measures for which E outperforms P.\n",
    "\n",
    "In order to produce a Simple Dynamic Bayesian Model based on our data, a number of assumptions were made. These assumptions were caused by data sparsity in the data set.\n",
    "Firstly, like in the Random Click Model, the documents and queries are assumed to be unique.\n",
    "Secondly, the sparseness of the data makes it impossible to calculate a satisfiablility score $\\sigma$ for each query and document. Instead, a general $\\sigma$ is calculated by finding the proportion of final clicks in a query to the total number of clicks. This method assumes that a user is satisfied after its final click in a query, meaning that any following queries in the session should be independent of this query. A general $\\sigma$ based on the number of last clicks in a query was also considered. This would assume dependency between queries in a session, meaning that the results of the previous query were unsatisfiable and a new query was entered to produce more satisfiable results. The first solution was more favourable, as we have no session information in our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Simple Dynamic Bayesian Model #####\n",
    "\n",
    "\n",
    "def learn_param_DBM(file):\n",
    "    '''This function learns the probability of a user being satisfied after\n",
    "    having clicked on a document using a click log\n",
    "    it takes training data as input and returns sigma (satisfaction)\n",
    "    '''\n",
    "        \n",
    "    lines = file.readlines()\n",
    "\n",
    "    #previous_session = 0 # Keep track of session number to determine if click is last click.\n",
    "    previous_type = \"\"\n",
    "    \n",
    "    clicks = 0\n",
    "    #last_clicks_session = 0\n",
    "    last_clicks_query = 0\n",
    "\n",
    "    lines.reverse() # Reversed order, so it is detectable if a click is last.\n",
    "    for line in lines:\n",
    "        items = re.split(r'\\t+',line) #strip tabs\n",
    "        current_type = items[2]\n",
    "\n",
    "        # check if event is a click and whether it was a last click\n",
    "        if current_type == \"C\" and previous_type == \"Q\": \n",
    "            last_clicks_query += 1\n",
    "        if current_type == \"C\":\n",
    "            clicks += 1\n",
    "\n",
    "        previous_type = current_type\n",
    "\n",
    "    sigma = last_clicks_query/clicks\n",
    "    \n",
    "    return sigma\n",
    "        \n",
    "\n",
    "\n",
    "def predict_prob_DBM(rank, sigma):\n",
    "    '''This function predicts a click probability\n",
    "    for the click probability, we'll need P(A) and P(E)\n",
    "    first get alpha, which will be set according to the level of relevance of a document\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # the following alpha's are based on a navigational model\n",
    "    if rank == 'HR':\n",
    "        alpha = 0.95\n",
    "    elif rank == 'R':\n",
    "        alpha = 0.5\n",
    "    elif rank == 'N':\n",
    "        alpha = 0.05 \n",
    "    \n",
    "    # check if user will click on the document (depending on alpha)\n",
    "    x = random.random()\n",
    "    if x <= alpha:\n",
    "        P_A = 1\n",
    "    else:\n",
    "        P_A = 0\n",
    "            \n",
    "    # since we are using a simple DBM, gamma will always be one    \n",
    "    gamma = 1\n",
    "    \n",
    "    return P_A, gamma  \n",
    "       \n",
    "        \n",
    "\n",
    "def click_doc_DBM(ranking, sigma):\n",
    "    '''This function decide which documents are clicked\n",
    "    this function takes a ranking list and a value for the parameter sigma as input and uses\n",
    "    these to determine which documents in the ranking list are clicked on\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # set P(E) to 1 (first snippet is always read)\n",
    "    P_E = 1\n",
    "    \n",
    "    clicked = []\n",
    "    \n",
    "    # run through the ranking to decide whether a document will be clicked or not\n",
    "    for rank in ranking:\n",
    "        P_A, gamma = predict_prob_DBM(rank, sigma)\n",
    "        \n",
    "        # based on probability, set click to 1 or 0\n",
    "        if P_A == 1 and P_E == 1:\n",
    "            P_C = 1\n",
    "        else:\n",
    "            P_C = 0\n",
    "     \n",
    "        clicked.append(P_C)\n",
    "        \n",
    "        # if user has clicked, check if user is satisfied\n",
    "        if P_C == 1:\n",
    "            # now check if user is satisfied\n",
    "            x = random.random()\n",
    "            if x <= sigma:\n",
    "                # if satisfied, user will not read any more snippets (thus click nothing)\n",
    "                P_E = 0\n",
    "            else:\n",
    "                # if user is not satisfied, user will read next snippet (thus possibly click)\n",
    "                P_E = 1 \n",
    "        \n",
    "    return clicked       \n",
    "\n",
    "\n",
    "def DBM_simulation(pairs, origins, sigma, N):\n",
    "    '''The following function represents the simulation of the dynamic bayesion model\n",
    "    \n",
    "    the function takes an interleaved ranking and a list which tells which element\n",
    "    in the ranking comes from which algorithm. Furthermore, parameters sigma and N\n",
    "    are given as input, representing satisfaction and number of simulations \n",
    "    respectively.\n",
    "    \n",
    "    output consists of a list consisting of N proportions where E\n",
    "    outperforms P for all pairs\n",
    "    '''\n",
    "    \n",
    "    # initiate output lists\n",
    "    p_DBM_list = []\n",
    "    list_wins = []\n",
    "    list_trials = []\n",
    "    \n",
    "    # simulate experiment N times\n",
    "    for i in range(N):\n",
    "\n",
    "        # keep track of which algorithm won\n",
    "        E_win = 0\n",
    "        P_win = 0\n",
    "        # loop through all rankings\n",
    "        for j, ranking in enumerate(pairs):\n",
    "\n",
    "            # get which documents were clicked\n",
    "            clicked = click_doc_DBM(ranking, sigma)\n",
    "\n",
    "            E_click = 0\n",
    "            P_click = 0\n",
    "\n",
    "            current_origin = origins[j]\n",
    "\n",
    "            # check whether the clicked documents were produced by E or P\n",
    "            for h, click in enumerate(clicked):\n",
    "                if click == 1 and current_origin[h] == 1:\n",
    "                    E_click += 1\n",
    "                elif click == 1 and current_origin[h] == 0:\n",
    "                    P_click += 1\n",
    "\n",
    "            # determine whether E or P won\n",
    "            if E_click > P_click:\n",
    "                E_win += 1\n",
    "            elif P_click > E_click:\n",
    "                P_win += 1\n",
    "       \n",
    "        \n",
    "        # proportion of times E won\n",
    "        if (E_win - P_win) == 0:\n",
    "            p = 0.5\n",
    "        else:\n",
    "            p = E_win / (E_win + P_win)\n",
    "            \n",
    "        p_DBM_list.append(p)\n",
    "        \n",
    "        list_wins.append(E_win)\n",
    "        list_trials.append((E_win + P_win))\n",
    "        \n",
    "    return p_DBM_list, list_wins, list_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Simulate dynamic bayesian model #####\n",
    "\n",
    "# get parameter out of data\n",
    "f=open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "sigma = learn_param_DBM(f)\n",
    "f.close()\n",
    "\n",
    "# run N simulations\n",
    "N = 50\n",
    "\n",
    "# get the p for the average of all metrics\n",
    "p_DBM_list, DBM_ALL_wins, DBM_ALL_trials = DBM_simulation(all_results, all_origins, sigma, N)\n",
    "\n",
    "# get the p for every metric \n",
    "p_AP_DBM_list, DBM_AP_wins, DBM_AP_trials = DBM_simulation(AP_results, AP_origins, sigma, N)\n",
    "p_DCG_DBM_list, DBM_DCG_wins, DBM_DCG_trials = DBM_simulation(DCG_results, DCG_origins, sigma, N)\n",
    "p_ERR_DBM_list, DBM_ERR_wins, DBM_ERR_trials = DBM_simulation(ERR_results, ERR_origins, sigma, N)\n",
    "\n",
    "print(p_DBM_list)\n",
    "print(p_AP_DBM_list)\n",
    "print(p_DCG_DBM_list)\n",
    "print(p_ERR_DBM_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the effect of different values for delta on proportion of wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new dict with all values for which delta is positive\n",
    "pairs_dict = {}\n",
    "for key, value in delta_values.items():\n",
    "    avg = sum(value, 0.0)/len(value)\n",
    "    if avg > 0.0:\n",
    "        pairs_dict[key] = avg\n",
    "\n",
    "keys = sorted(pairs_dict, key=pairs_dict.get)\n",
    "\n",
    "index = round(len(pairs_dict)/10)\n",
    "\n",
    "##### set parameters #######\n",
    "\n",
    "# get rho out of data\n",
    "f=open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "rho = learn_param_RCM(f)\n",
    "f.close()\n",
    "\n",
    "# get sigma out of data                     \n",
    "f=open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "sigma = learn_param_DBM(f)\n",
    "f.close()\n",
    "\n",
    "# take first 10 % of the sorted dictionary to represent the low delta values\n",
    "low_deltas = keys[:index]\n",
    "\n",
    "# take last 10 % of the sorted dictionary to represent the high delta values\n",
    "high_deltas = keys[-index:]\n",
    "\n",
    "# interleave both pair lists\n",
    "low_delta_results, low_delta_origins = interleaving(low_deltas)\n",
    "high_delta_results, high_delta_origins = interleaving(high_deltas)\n",
    "\n",
    "# run the random click model on both delta's\n",
    "p_RCM_ld, _,_ = RCM_simulation(low_delta_results, low_delta_origins, rho, N)\n",
    "p_RCM_hd, _,_ = RCM_simulation(high_delta_results, high_delta_origins, rho, N)\n",
    "\n",
    "# run the dynamic bayesion model on both delta's\n",
    "p_DBM_ld, _,_ = DBM_simulation(low_delta_results, low_delta_origins, sigma, N)\n",
    "p_DBM_hd, _,_ = DBM_simulation(high_delta_results, high_delta_origins, sigma, N)\n",
    "\n",
    "# run statistical tests to find significant differences\n",
    "scipy.stats.ttest_ind(p_DBM_ld, p_DBM_hd)\n",
    "\n",
    "####### CREATE PLOT ########\n",
    "\n",
    "x = [0.25,0.8]\n",
    "names = ['RCM', 'DBM']\n",
    "\n",
    "bars_lowdelta = [sum(p_RCM_ld)/len(p_RCM_ld), sum(p_DBM_ld)/len(p_DBM_ld)]\n",
    "bars_highdelta = [sum(p_RCM_hd)/len(p_RCM_hd), sum(p_DBM_hd)/len(p_DBM_hd)]\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(x[0]-0.1, bars_lowdelta[0] ,width=0.2,color='dodgerblue',align='center', label = 'low delta')\n",
    "ax.bar(x[0]+0.1, bars_lowdelta[1] ,width=0.2,color='tomato',align='center', label = 'high delta')\n",
    "ax.bar(x[1]-0.1, bars_highdelta[0] ,width=0.2,color='dodgerblue',align='center')\n",
    "ax.bar(x[1]+0.1, bars_highdelta[1] ,width=0.2,color='tomato',align='center')\n",
    "ax.set_xticks([x[0],x[1]])\n",
    "ax.set_xticklabels(names)\n",
    "plt.ylabel('Proportion of wins for E')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels)\n",
    "ax.set_title('Effect of delta values on click models')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure ?: Effect of delta values on click models. This figure shows the proportion of times where algorithm E provided more links that were clicked than algorithm P in a ranking. The blue bars represent 10% of the delta values where E outperforms P  just slightly (lowest delta values). The red bars represent 10% of the delta values where E outperforms P more clearly (highest delta values). The delta values where calculated by taking the mean of all results of the offline measures per pair. \n",
    "\n",
    "From this figure can be concluded that the models as well as the offline metrics seem to work. Pairs are extracted based on their delta measure. Pairs are only considered if E outperforms P (resulting in extraction of all positive delta values). The higher the delta value, the more E outperforms P, meaning the more relevant documents will be provided by E in comparison with P. We thus expected to find differences in only our Dynamic Bayesian Model based on these delta values. For the Random Click Model we did not expect to find any differences, since the probability of clicking on a document does not depend in any way on the relevance of a document. Thus, the manner in which E outperform P (meaning having more relevant documents in an interleaved ranking) will not have any effect on what is clicked.\n",
    "For the Dynamic Bayesian Model (DBM), we did expect to find differences caused by the difference in delta values. We based this expectation on the fact that in a DBM, relevance has an impact on click probability. Therefore, algorithms that provide more relevant documents in comparison to other algorithms in a ranking will be more likely to win (provide more documents that are clicked on). This effect can be found in the figure above, which shows that within the DBM, E wins by a larger margin if it provides more relevant documents in comparison to P (thus having a larger delta value for the offline measures) . If E outperforms P just slightly (by having a low delta value), E is not more likely to provide more documents that were clicked on, since the algorithms provide roughly the same amount of relevant documents. These results confirm that our offline measures calculate their delta in such a way that it has a high value for pairs where E outperforms P by a large margin and a low value for pairs where E outperforms P by a small margin. Moreover, we can conclude that our click models seem to work based on the assumptions that we made beforehand and the results shown in this figure. The RCM is not affected by difference in relevance of provided documents, while the DBM shows that an algorithm that provides relatively more relevant documents is more likely to win (provide more documents that are clicked) than an algorithm that provides relatively less relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### PLOT OVER THE AVERAGE OF ALL METRICS #####\n",
    "\n",
    "# average over all metrics and standard deviation for random click model\n",
    "avg_ALL_RCM = statistics.mean(p_RCM_list)\n",
    "stdev_ALL_RCM = statistics.pstdev(p_RCM_list)\n",
    "\n",
    "# average over all metrics and standard deviation for dynamic bayesian model\n",
    "avg_ALL_DBM = statistics.mean(p_DBM_list)\n",
    "stdev_ALL_DBM = statistics.pstdev(p_DBM_list)\n",
    "\n",
    "# plot bar graph over all metrics, RCM compared to DBM\n",
    "ax = plt.gca()\n",
    "\n",
    "x = (1, 1.75)\n",
    "y = (avg_ALL_RCM, avg_ALL_DBM)\n",
    "z = (stdev_ALL_RCM, stdev_ALL_DBM)\n",
    "names = ('RCM', 'DBM')\n",
    "clr = ('tomato', 'dodgerblue')\n",
    "for i in range(len(x)):\n",
    "    plt.bar(x[i], y[i], width=0.5, color = clr[i], yerr = z[i], align=\"center\")\n",
    "\n",
    "ind = (1,1.75)\n",
    "plt.xticks(ind, x)\n",
    "plt.ylabel('Proportion of wins for E')\n",
    "ax.set_xticklabels(names)\n",
    "\n",
    "leg = ax.legend(names, loc = 'upper right')\n",
    "ax.set_title('Average over all metrics for both the Random Click Model (RCM) and the Dynamic Bayesian Model')\n",
    "\n",
    "# Get the bounding box of the original legend\n",
    "bb = leg.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "\n",
    "# Change the location of the legend. \n",
    "xOffset = 0.25\n",
    "bb.x0 += xOffset\n",
    "bb.x1 += xOffset\n",
    "leg.set_bbox_to_anchor(bb, transform = ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above merely implies that our implemented click models seem to work. We used the offline metrics to determine which pairs show that algorithm E provides more relevant documents in comparison to algorithm P. Afterwards, we used these pairs in our click model and simulated the model. \n",
    "\n",
    "We expect to find that within the Random Click Model (RCM), no offline metric will significantly differ from 0.5 (based on the plot showing the proportion of wins after taking the average delta of all offline metrics). Thus, the fact that E provides more relevant documents has no effect on its proportion of wins compared to P (50/50) in a RCM. This could be due to the fact that a RCM does not take the relevancy into account. \n",
    "\n",
    "When it comes to the Dynamic Bayesian Model (DBM), we expect to find that all metrics will significantly differ from 0.5. We base this expectation on the fact that a DBM takes relevancy into account. As we only simulate the pairs where E has more relevant documents compared to P, E is more likely to provide more documents that will be clicked on. Thus the proportion of wins for E will be significantly higher than 0.5.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "We will use a binomial test to check for significant differences. Our null hypothesis is that E will not outperform P and thus have a win proportion of about 0.5, whereas our alternative hypothesis is that E does indeed outperform P and thus have a win proportion that is significantly higher than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### PLOTS FOR EACH METRIC #####\n",
    "\n",
    "# average and standard deviation per metric for random click model\n",
    "avg_AP_RCM = statistics.mean(p_AP_RCM_list)\n",
    "stdev_AP_RCM = statistics.pstdev(p_AP_RCM_list)\n",
    "\n",
    "avg_DCG_RCM = statistics.mean(p_DCG_RCM_list)\n",
    "stdev_DCG_RCM = statistics.pstdev(p_DCG_RCM_list)\n",
    "\n",
    "avg_ERR_RCM = statistics.mean(p_ERR_RCM_list)\n",
    "stdev_ERR_RCM = statistics.pstdev(p_ERR_RCM_list)\n",
    "\n",
    "# average and standard deviation per metric for random click model\n",
    "avg_AP_DBM = statistics.mean(p_AP_DBM_list)\n",
    "stdev_AP_DBM = statistics.pstdev(p_AP_DBM_list)\n",
    "\n",
    "avg_DCG_DBM = statistics.mean(p_DCG_DBM_list)\n",
    "stdev_DCG_DBM = statistics.pstdev(p_DCG_DBM_list)\n",
    "\n",
    "avg_ERR_DBM = statistics.mean(p_ERR_DBM_list)\n",
    "stdev_ERR_DBM = statistics.pstdev(p_ERR_DBM_list)\n",
    "\n",
    "\n",
    "# plot bar graph over all metrics, RCM compared to DBM\n",
    "ax = plt.gca()\n",
    "\n",
    "#x = range(1, 7)\n",
    "x = (2,2,2,4,4,4)\n",
    "y = (avg_AP_RCM, avg_DCG_RCM, avg_ERR_RCM, avg_AP_DBM, avg_DCG_DBM, avg_ERR_DBM)\n",
    "z = (stdev_AP_RCM, stdev_DCG_RCM, stdev_ERR_RCM, stdev_AP_DBM, stdev_DCG_DBM, stdev_ERR_DBM)\n",
    "names_leg = ('AP', 'DCG', 'ERR', 'AP', 'DCG', 'ERR')\n",
    "names = ('RCM', 'DBM')\n",
    "clr = ('r', 'tomato', 'lightsalmon', 'b', 'dodgerblue', 'lightskyblue')\n",
    "sides = (0.5, 0, -0.5, 0.5, 0, -0.5)\n",
    "for i in range(len(x)):\n",
    "    plt.bar(x[i]-sides[i], y[i], width=0.5, color = clr[i], yerr = z[i], align=\"center\")\n",
    "\n",
    "ind = (2,4)\n",
    "plt.xticks(ind, x)\n",
    "plt.ylabel('Proportion of wins for E')\n",
    "ax.set_xticklabels(names)\n",
    "\n",
    "leg = ax.legend(names_leg, loc = 'upper right')\n",
    "ax.set_title('Average proportion of wins for both models')\n",
    "\n",
    "# Get the bounding box of the original legend\n",
    "bb = leg.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "\n",
    "# Change the location of the legend. \n",
    "xOffset = 0.25\n",
    "bb.x0 += xOffset\n",
    "bb.x1 += xOffset\n",
    "leg.set_bbox_to_anchor(bb, transform = ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure ?: Average proportion of wins for both models. The figure above shows the proportion of wins for E, meaning how often algorithm E provided more documents that were clicked on in comparison to algorithm P. The red bars indicate win proportions for the Random Click Model (RCM), whereas the blue bars indicate win proportions for the Dynamic Bayesian Model (DBM). Different shades of the colours represent the offline metrics used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### STATISTICAL ANALYSIS #####\n",
    "\n",
    "def calculate_pval(wins, trials):\n",
    "    ''' This function calculates the p value after performing a binomial test\n",
    "    it takes the amount of wins and the amount of trials from a simulation\n",
    "    and performs a binomial test using these parameters\n",
    "    '''\n",
    "    \n",
    "    success = 0\n",
    "    \n",
    "    for i in range(len(wins)):\n",
    "        # check if the percentage of wins is greater than 50%\n",
    "        if wins[i]/trials[i] > 0.5:\n",
    "            success += 1\n",
    "\n",
    "    # binomial test using the number of successful runs (above 50%)\n",
    "    # and the number of runs in total to get a p-value\n",
    "    pval = scipy.stats.binom_test(success, len(trials), p=0.5)\n",
    "    \n",
    "    return pval\n",
    "    \n",
    "##### Check which way we want #####\n",
    "\n",
    "# statistical test over all metrics combined\n",
    "pval_RCM_ALL = calculate_pval(RCM_ALL_wins, RCM_ALL_trials)\n",
    "pval_DBM_ALL = calculate_pval(DBM_ALL_wins, DBM_ALL_trials)\n",
    "\n",
    "# statistical test over AP\n",
    "pval_RCM_AP = calculate_pval(RCM_AP_wins, RCM_AP_trials)\n",
    "pval_DBM_AP = calculate_pval(DBM_AP_wins, DBM_AP_trials)\n",
    "\n",
    "# statistical test over DCG\n",
    "pval_RCM_DCG = calculate_pval(RCM_DCG_wins, RCM_DCG_trials)\n",
    "pval_DBM_DCG = calculate_pval(DBM_DCG_wins, DBM_DCG_trials)\n",
    "\n",
    "# statistical test over ERR\n",
    "pval_RCM_ERR = calculate_pval(RCM_ERR_wins, RCM_ERR_trials)\n",
    "pval_DBM_ERR = calculate_pval(DBM_ERR_wins, DBM_ERR_trials)\n",
    "\n",
    "print('Binomial tests to compare whether proportion differs from 0.5:')\n",
    "print('\\n')\n",
    "print('P-value for RCM all: ', pval_RCM_ALL)\n",
    "print('P-value for DBM all: ', pval_DBM_ALL)\n",
    "print('\\n')\n",
    "print('P-value for RCM Average Precision: ', pval_RCM_AP)\n",
    "print('P-value for RCM DCG: ', pval_RCM_DCG)\n",
    "print('P-value for RCM ERR: ', pval_RCM_ERR)\n",
    "print('\\n')\n",
    "print('P-value for DBM AP: ', pval_DBM_AP)\n",
    "print('P-value for DBM DCG: ', pval_DBM_DCG)\n",
    "print('P-value for DBM ERR: ', pval_DBM_ERR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significance level: $\\alpha = 0.05$\n",
    "\n",
    "The p-values shown above are the result of running a binomial test on all combinations of online and offline metrics. The binomial test was run to test whether the win proportion deviated from 0.5, meaning whether E would outperform P. In the Random Click Model (RCM) it can be seen that none of the offline metrics leads to a significant difference from 0.5. Therefore, it can be concluded that within a RCM E does not seem to outperform P and we keep our null hypothesis. This makes sense, as the RCM does not base its click probability on relevancy of documents. The offline metrics were used to determine which pairs had more relevant documents from E than from P. As the relevancy is not taken into account in the RCM, we did not expect to find any significant differences with this test on the model.\n",
    "\n",
    "However, when it comes to the Dynamic Bayesian Model (DBM) we find that all offline metrics show significant results. This means that within the DBM, all offline metrics show that algorithm E has a significantly higher proportion of wins compared to algorithm P. Hence, for DBM we reject our null hypothesis. Beforehand, we expected all these values to be significantly different based on the fact that DBM uses document relevance in order to calculate click probability. Thus, the model is more likely to click on a document that is labeled as being more relevant. We used the offline measures to determine which pairs show that algorithm E provided more relevant documents in comparison with algorithm P. These pairs where then used in the click model, where documents provided by E would be clicked on more just based on the fact that they are more relevant. Due to the fact that we find significant differences on the binomial test on all offline measures, we conclude that documents provided by E are clicked more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences within models\n",
    "For further analysis, we decided to check whether the offline metric that was used has an effect on the proportion of wins within both models. For this analysis we decided to use a t-test to find whether or not the proportion of wins differ between the use of different offline metrics within the click models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### T-TEST #####\n",
    "\n",
    "print('Random Click Model analysis:')\n",
    "stat, pval_AP_DCG_RCM = scipy.stats.ttest_ind(p_AP_RCM_list, p_DCG_RCM_list)\n",
    "print('T-test p-value between AP and DCG: ', pval_AP_DCG_RCM)\n",
    "\n",
    "stat, pval_AP_ERR_RCM = scipy.stats.ttest_ind(p_AP_RCM_list, p_ERR_RCM_list)\n",
    "print('T-test p-value between AP and ERR: ', pval_AP_ERR_RCM)\n",
    "\n",
    "stat, pval_DCG_ERR_RCM = scipy.stats.ttest_ind(p_DCG_RCM_list, p_ERR_RCM_list)\n",
    "print('T-test p-value between DCG and ERR: ', pval_DCG_ERR_RCM)\n",
    "print('\\n')\n",
    "\n",
    "print('Dynamic Bayesian Model analysis:')\n",
    "stat, pval_AP_DCG_DBM = scipy.stats.ttest_ind(p_AP_DBM_list, p_DCG_DBM_list)\n",
    "print('T-test p-value between AP and DCG: ', pval_AP_DCG_DBM)\n",
    "\n",
    "stat, pval_AP_ERR_DBM = scipy.stats.ttest_ind(p_AP_DBM_list, p_ERR_DBM_list)\n",
    "print('T-test p-value between AP and ERR: ', pval_AP_ERR_DBM)\n",
    "\n",
    "stat, pval_DCG_ERR_DBM = scipy.stats.ttest_ind(p_DCG_DBM_list, p_ERR_DBM_list)\n",
    "print('T-test p-value between DCG and ERR: ', pval_DCG_ERR_DBM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significance level: $\\alpha = 0.05$\n",
    "\n",
    "As expected, we do not find any significant differences within the Random Click Model (RCM). As mentioned before, this is due to the fact that a RCM does not take relevancy of documents in a ranking into account. This is in line with the results shown in the graph concerning RCM, where differences between proportions of wins are minor.\n",
    "\n",
    "For the Dynamic Bayesian Model (DBM), we find that every pair seems to significantly differ from one another. The p-values are extremely low, most likely due to the fact that our data has a very small standard deviation. By looking at the graph we find that DCG performs best, Average Precision (AP) comes in at second and ERR performs worst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Simulate experiments on single data points #####\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get delta values for all offline metrics\n",
    "deltas_AP = []\n",
    "for pair in AP_pairs:\n",
    "    deltas_AP.append(AP_delta[pair])\n",
    "deltas_AP = np.squeeze(deltas_AP)\n",
    "    \n",
    "deltas_DCG = []\n",
    "for pair in DCG_pairs:\n",
    "    deltas_DCG.append(nDCG_delta[pair])\n",
    "deltas_DCG = np.squeeze(deltas_DCG)\n",
    "deltas_DCG = deltas_DCG/5 # Normalize over length of list\n",
    "\n",
    "deltas_ERR = []    \n",
    "for pair in ERR_pairs:\n",
    "    deltas_ERR.append(ERR_delta[pair])\n",
    "deltas_ERR = np.squeeze(deltas_ERR)\n",
    "    \n",
    "# get rho out of data\n",
    "f=open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "rho = learn_param_RCM(f)\n",
    "f.close()\n",
    "\n",
    "# get sigma out of data                     \n",
    "f=open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "sigma = learn_param_DBM(f)\n",
    "f.close()\n",
    "\n",
    "# Simulations for AP\n",
    "AP_RCM_win_probs = []\n",
    "AP_DBM_win_probs = []\n",
    "for pair in AP_pairs:\n",
    "    RCM_pair_wins = []\n",
    "    DBM_pair_wins = []\n",
    "    for i in range(50):    \n",
    "        result, origin = interleaving([pair])\n",
    "        p_RCM_ld, _, _ = RCM_simulation(result, origin, rho, 1)\n",
    "        RCM_pair_wins.append(p_RCM_ld)\n",
    "        p_DBM_ld, _,_ = DBM_simulation(result, origin, sigma, 1)\n",
    "        DBM_pair_wins.append(p_DBM_ld)\n",
    "    \n",
    "    # Calculate proportions of wins for a single pair and add to list\n",
    "    RCM_pair_wins = np.squeeze(RCM_pair_wins)\n",
    "    RCM_win_prob = sum(RCM_pair_wins, 0.0)/len(RCM_pair_wins)\n",
    "    AP_RCM_win_probs.append(RCM_win_prob)\n",
    "    DBM_pair_wins = np.squeeze(DBM_pair_wins)\n",
    "    DBM_win_prob = sum(DBM_pair_wins, 0.0)/len(DBM_pair_wins)\n",
    "    AP_DBM_win_probs.append(DBM_win_prob)\n",
    "\n",
    "# Simulations for DCG\n",
    "DCG_RCM_win_probs = []\n",
    "DCG_DBM_win_probs = []\n",
    "for pair in DCG_pairs:\n",
    "    RCM_pair_wins = []\n",
    "    DBM_pair_wins = []\n",
    "    for i in range(50):    \n",
    "        result, origin = interleaving([pair])\n",
    "        p_RCM_ld, _, _ = RCM_simulation(result, origin, rho, 1)\n",
    "        RCM_pair_wins.append(p_RCM_ld)\n",
    "        p_DBM_ld, _,_ = DBM_simulation(result, origin, sigma, 1)\n",
    "        DBM_pair_wins.append(p_DBM_ld)\n",
    "    \n",
    "    # Calculate proportions of wins for a single pair and add to list\n",
    "    RCM_pair_wins = np.squeeze(RCM_pair_wins)\n",
    "    RCM_win_prob = sum(RCM_pair_wins, 0.0)/len(RCM_pair_wins)\n",
    "    DCG_RCM_win_probs.append(RCM_win_prob)\n",
    "    DBM_pair_wins = np.squeeze(DBM_pair_wins)\n",
    "    DBM_win_prob = sum(DBM_pair_wins, 0.0)/len(DBM_pair_wins)\n",
    "    DCG_DBM_win_probs.append(DBM_win_prob)\n",
    "\n",
    "\n",
    "# Simulations for ERR\n",
    "ERR_RCM_win_probs = []\n",
    "ERR_DBM_win_probs = []\n",
    "for pair in ERR_pairs:\n",
    "    RCM_pair_wins = []\n",
    "    DBM_pair_wins = []\n",
    "    for i in range(50):    \n",
    "        result, origin = interleaving([pair])\n",
    "        p_RCM_ld, _, _ = RCM_simulation(result, origin, rho, 1)\n",
    "        RCM_pair_wins.append(p_RCM_ld)\n",
    "        p_DBM_ld, _,_ = DBM_simulation(result, origin, sigma, 1)\n",
    "        DBM_pair_wins.append(p_DBM_ld)\n",
    "    \n",
    "    # Calculate proportions of wins for a single pair and add to list\n",
    "    RCM_pair_wins = np.squeeze(RCM_pair_wins)\n",
    "    RCM_win_prob = sum(RCM_pair_wins, 0.0)/len(RCM_pair_wins)\n",
    "    ERR_RCM_win_probs.append(RCM_win_prob)\n",
    "    DBM_pair_wins = np.squeeze(DBM_pair_wins)\n",
    "    DBM_win_prob = sum(DBM_pair_wins, 0.0)/len(DBM_pair_wins)\n",
    "    ERR_DBM_win_probs.append(DBM_win_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create plots based on proportions per pair found above #####\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, axarr = plt.subplots(2,3, figsize=(20,10))\n",
    "extent = [0,1,0,1]\n",
    "\n",
    "# RCM AP\n",
    "heatmap, xedges, yedges = np.histogram2d(deltas_AP, AP_RCM_win_probs, bins=(20,20))\n",
    "axarr[0,0].imshow(heatmap.T, extent=extent, origin='lower')\n",
    "axarr[0,0].set_title(\"Average Precision on Random Click Model\")\n",
    "\n",
    "# RCM DCG\n",
    "heatmap, xedges, yedges = np.histogram2d(deltas_DCG/5, DCG_RCM_win_probs, bins=(20,20))\n",
    "axarr[0,1].imshow(heatmap.T, extent=extent, origin='lower')\n",
    "axarr[0,1].set_title(\"Discounted Cumulative Gain on Random Click Model\")\n",
    "\n",
    "# RCM ERR\n",
    "heatmap, xedges, yedges = np.histogram2d(deltas_ERR, ERR_RCM_win_probs, bins=(20,20))\n",
    "axarr[0,2].imshow(heatmap.T, extent=extent, origin='lower')\n",
    "axarr[0,2].set_title(\"Expected Reciprocal Rank on Random Click Model\")\n",
    "\n",
    "# DBM AP\n",
    "heatmap, xedges, yedges = np.histogram2d(deltas_AP, AP_DBM_win_probs, bins=(20,20))\n",
    "axarr[1,0].imshow(heatmap.T, extent=extent, origin='lower')\n",
    "axarr[1,0].set_title(\"Average Precision on Dynamic Bayesian Model\")\n",
    "\n",
    "# DBM DCG\n",
    "heatmap, xedges, yedges = np.histogram2d(deltas_DCG/5, DCG_DBM_win_probs, bins=(20,20))\n",
    "axarr[1,1].imshow(heatmap.T, extent=extent, origin='lower')\n",
    "axarr[1,1].set_title(\"Discounted Cumulative Gain on Dynamic Bayesian Model\")\n",
    "\n",
    "# DBM ERR\n",
    "heatmap, xedges, yedges = np.histogram2d(deltas_ERR, ERR_DBM_win_probs, bins=(20,20))\n",
    "axarr[1,2].imshow(heatmap.T, extent=extent, origin='lower')\n",
    "axarr[1,2].set_title(\"Expected Reciprocal Rank on Dynamic Bayesian Model\")\n",
    "\n",
    "# Set attributes for all subplots\n",
    "for x in range(2):\n",
    "    for y in range(3):\n",
    "        axarr[x,y].set_xlim([0,1])\n",
    "        axarr[x,y].set_ylim([0,1])\n",
    "        axarr[x,y].set_xlabel(\"Delta Score (offline)\")\n",
    "        axarr[x,y].set_ylabel(\"Win Proportion (online)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these heatmaps we compare the offline metrics to the online performance in both the Click Models. Yellow indicates a higher occurence of a combination of values, while dark blue indicates that there are no combinations of these values. The proportion of wins is calculated after simulating the click model for each single data point 100 times.\n",
    "\n",
    "It becomes clear that the heatmaps of the offline metrics in the Random Click Model are very similar. This is in line with our previous results, as offline metrics do not influence this model. In this way, they differ from the results on the Dynamic Bayesian Model.\n",
    "There are visible differences between the offline measures on the Dynamic Basyesian Model. DCG shows a clear correlation between the delta score and the win proportion. This correlation is less clear in Average Precision and Expected Reciprocal Rank. In this last measure, it should be noted that most delta scores are slightly higher than zero meaning there is a small difference between the two rankings.\n",
    "These findings are in line with our previous results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Through our test of the evaluation measures, we find that DCG performs best on our data, AP has the second-best performance and ERR performs the worst.\n",
    "\n",
    "A likely cause of the relatively poor performance by ERR is the assumption of independence between queries and documents (which is an assumption that we made within this experiment). ERR is an algorithm designed specifically to tackle the downsides of this assumption and is thus likely to perform better in a real-world application. \n",
    "\n",
    "DCG performs better than AP due to the fact that in this implementation of AP the relevance is binary and no disjunction is made between documents that are \"relevant\" and documents that are \"highly relevant\".\n",
    "\n",
    "Finally, we have shown that an effect occurs between the delta measures and the proportion of wins on the dynamic bayesian model. This effect seems to be biggest when using the DCG measure, which is in line with what can be seen in figure 2. This effect does not seem to occur on the random click model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
